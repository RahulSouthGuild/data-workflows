# Week 1: Config Foundation - Detailed Implementation Plan

> **Goal:** Create the complete configuration infrastructure that will serve as the foundation for multi-tenant ETL pipeline
> **Duration:** 5 days (Monday - Friday)
> **Output:** Fully functional config system that can be loaded and validated

---

## Day 1 (Monday): Directory Structure + Tenant Registry

### ğŸ¯ Objective
Create the basic config directory structure and the master tenant registry file.

### ğŸ“ Directory Structure to Create

```bash
configs/
â”œâ”€â”€ tenant_registry.yaml          # Master list of all tenants
â”œâ”€â”€ shared/                        # Shared defaults
â”‚   â”œâ”€â”€ default_config.yaml
â”‚   â””â”€â”€ common_business_rules.yaml
â”œâ”€â”€ starrocks/                     # StarRocks-specific configs
â”‚   â”œâ”€â”€ connection_pool.yaml
â”‚   â””â”€â”€ stream_load_defaults.yaml
â””â”€â”€ tenants/                       # Per-tenant configurations
    â”œâ”€â”€ _template/                 # Template for new tenants
    â”‚   â”œâ”€â”€ config.yaml.template
    â”‚   â”œâ”€â”€ .env.template
    â”‚   â”œâ”€â”€ schemas/
    â”‚   â”‚   â”œâ”€â”€ tables/
    â”‚   â”‚   â”œâ”€â”€ views/
    â”‚   â”‚   â””â”€â”€ matviews/
    â”‚   â”œâ”€â”€ column_mappings/
    â”‚   â”œâ”€â”€ computed_columns.yaml
    â”‚   â”œâ”€â”€ business_logic/
    â”‚   â”‚   â”œâ”€â”€ business_constants.py
    â”‚   â”‚   â”œâ”€â”€ validation_rules.py
    â”‚   â”‚   â””â”€â”€ rls_config.py
    â”‚   â””â”€â”€ seeds/
    â”‚       â””â”€â”€ SEED_MAPPING.py
    â””â”€â”€ tenant1/                   # First tenant (current system)
        â””â”€â”€ (same structure as _template)
```

### ğŸ“ Step-by-Step Tasks

#### Task 1.1: Create Directory Structure
```bash
# Execute from project root
cd /home/rahul/RahulSouthGuild/data-workflows

# Create main configs directory
mkdir -p configs/{shared,starrocks,tenants/{_template,tenant1}}

# Create _template subdirectories
mkdir -p configs/tenants/_template/{schemas/{tables,views,matviews},column_mappings,business_logic,seeds}

# Create tenant1 subdirectories (mirror of _template)
mkdir -p configs/tenants/tenant1/{schemas/{tables,views,matviews},column_mappings,business_logic,seeds}

# Verify structure
tree configs/ -L 3
```

#### Task 1.2: Create `tenant_registry.yaml`

**File:** `configs/tenant_registry.yaml`

```yaml
# ==============================================================================
# Pidilite DataWiz - Multi-Tenant Registry
# ==============================================================================
# This is the SINGLE SOURCE OF TRUTH for all tenant configurations.
#
# Purpose:
#   - Lists all tenants (enabled and disabled)
#   - Controls which tenants are processed by job runner
#   - Defines tenant metadata and priorities
#
# Last Updated: 2025-01-15
# ==============================================================================

# Tenant Definitions
# Each tenant represents a separate client/organization with isolated data
tenants:
  # ============================================================================
  # TENANT 1: Pidilite Mumbai Operations (Current Production System)
  # ============================================================================
  - tenant_id: tenant1
    tenant_name: "Pidilite Mumbai Operations"

    # Status
    enabled: true                           # Set to false to disable processing

    # Database Configuration
    database_name: "datawiz_tenant1"       # Separate database per tenant
    database_user: "tenant1_admin"         # Dedicated user for isolation

    # Azure Blob Storage
    azure_container: "pidilite-mumbai-prod"
    azure_folder_prefix: "synapse_data/"

    # Scheduling
    schedule_priority: 1                    # Lower number = higher priority
    tier: "premium"                         # premium, standard, basic

    # Metadata
    created_at: "2025-01-15"
    contact_email: "mumbai-admin@pidilite.com"

    # Notes
    notes: "Main production tenant - migrated from single-tenant system"

  # ============================================================================
  # TENANT 2: Pidilite Delhi Operations (Future)
  # ============================================================================
  - tenant_id: tenant2
    tenant_name: "Pidilite Delhi Operations"

    # Status
    enabled: false                          # Not yet onboarded

    # Database Configuration
    database_name: "datawiz_tenant2"
    database_user: "tenant2_admin"

    # Azure Blob Storage
    azure_container: "pidilite-delhi-prod"
    azure_folder_prefix: "data_exports/"

    # Scheduling
    schedule_priority: 2
    tier: "standard"

    # Metadata
    created_at: "2025-01-20"               # Expected onboarding date
    contact_email: "delhi-admin@pidilite.com"

    # Notes
    notes: "Planned for Q1 2025 onboarding"

  # ============================================================================
  # TENANT 3: Pidilite South Region (Future)
  # ============================================================================
  - tenant_id: tenant3
    tenant_name: "Pidilite South Region"

    # Status
    enabled: false                          # Not yet onboarded

    # Database Configuration
    database_name: "datawiz_tenant3"
    database_user: "tenant3_admin"

    # Azure Blob Storage
    azure_container: "pidilite-south-prod"
    azure_folder_prefix: "synapse_exports/"

    # Scheduling
    schedule_priority: 3
    tier: "standard"

    # Metadata
    created_at: "2025-02-01"               # Expected onboarding date
    contact_email: "south-admin@pidilite.com"

    # Notes
    notes: "Planned for Q1 2025 onboarding"


# ==============================================================================
# Global Configuration
# ==============================================================================
# These settings apply to ALL tenants and control the job runner behavior
global_config:
  # Execution Settings
  max_concurrent_tenants: 1                 # Sequential execution (1 tenant at a time)
  tenant_timeout: 7200                      # Max time per tenant in seconds (2 hours)
  fail_fast: false                          # Continue to next tenant on failure

  # Infrastructure
  shared_starrocks_cluster: true            # All tenants share same StarRocks cluster

  # Retry Settings
  max_retries: 3                            # Global retry limit
  retry_delay: 5                            # Delay between retries (seconds)

  # Logging
  log_level: "INFO"                         # DEBUG, INFO, WARNING, ERROR
  log_retention_days: 30                    # Keep logs for 30 days

  # Monitoring
  enable_observability: true                # Enable OpenTelemetry tracing
  health_check_interval: 300                # Health check every 5 minutes


# ==============================================================================
# Maintenance & Operational Notes
# ==============================================================================
# To onboard a new tenant:
#   1. Add entry above with enabled: false
#   2. Copy configs/tenants/_template to configs/tenants/tenant_new
#   3. Edit config.yaml and .env in tenant_new/
#   4. Create database: CREATE DATABASE datawiz_tenant_new;
#   5. Run: python db/create_tables.py --tenant tenant_new
#   6. Set enabled: true in this file
#
# To disable a tenant temporarily:
#   - Set enabled: false (jobs will skip this tenant)
#
# To remove a tenant permanently:
#   - Set enabled: false
#   - Back up data
#   - Drop database
#   - Archive configs/tenants/tenant_id/
# ==============================================================================
```

**Why This Structure:**
- âœ… **Self-documenting** - Extensive comments explain everything
- âœ… **Future-proof** - tenant2 and tenant3 already defined but disabled
- âœ… **Operational notes** - Built-in documentation for common tasks
- âœ… **Metadata tracking** - Created dates, contact emails for accountability

#### Task 1.3: Create `.gitignore` Update

**File:** `.gitignore` (append these lines)

```bash
# Multi-tenant secrets - NEVER commit!
configs/tenants/*/.env
configs/tenants/*/business_logic/*.secrets.py

# Tenant-specific generated files
configs/tenants/*/temp/
configs/tenants/*/.cache/
```

#### Task 1.4: Validation Script

Create a quick validation script to test the YAML structure:

**File:** `scripts/validate_registry.py`

```python
#!/usr/bin/env python3
"""
Validate tenant_registry.yaml structure
"""
import yaml
from pathlib import Path

def validate_registry():
    registry_path = Path("configs/tenant_registry.yaml")

    if not registry_path.exists():
        print("âŒ tenant_registry.yaml not found!")
        return False

    try:
        with open(registry_path) as f:
            registry = yaml.safe_load(f)

        # Check required keys
        assert "tenants" in registry, "Missing 'tenants' key"
        assert "global_config" in registry, "Missing 'global_config' key"

        # Check each tenant
        for tenant in registry["tenants"]:
            required = ["tenant_id", "tenant_name", "enabled", "database_name"]
            for key in required:
                assert key in tenant, f"Tenant missing '{key}': {tenant.get('tenant_id', 'unknown')}"

        print(f"âœ… tenant_registry.yaml is valid!")
        print(f"   - Total tenants: {len(registry['tenants'])}")
        print(f"   - Enabled: {sum(1 for t in registry['tenants'] if t['enabled'])}")
        print(f"   - Disabled: {sum(1 for t in registry['tenants'] if not t['enabled'])}")

        return True

    except Exception as e:
        print(f"âŒ Validation failed: {e}")
        return False

if __name__ == "__main__":
    validate_registry()
```

**Run:**
```bash
python scripts/validate_registry.py
```

---

## Day 2 (Tuesday): Shared Configs + StarRocks Configs

### ğŸ¯ Objective
Create shared configuration files that apply to all tenants.

### ğŸ“ Tasks

#### Task 2.1: Create `shared/default_config.yaml`

**File:** `configs/shared/default_config.yaml`

```yaml
# ==============================================================================
# Default Configuration - Applies to ALL Tenants
# ==============================================================================
# These settings are inherited by all tenants unless overridden in
# their individual config.yaml files.
#
# Inheritance Chain:
#   1. This file (lowest priority)
#   2. configs/tenants/{tenant_id}/config.yaml (overrides)
#   3. configs/tenants/{tenant_id}/.env (secrets override)
# ==============================================================================

# Database Defaults
database:
  host: "localhost"                         # Override in production
  port: 9030                                # StarRocks MySQL protocol port
  http_port: 8040                           # StarRocks HTTP port
  charset: "utf8mb4"

  # Connection Pool Settings
  pool_size: 10
  max_overflow: 20
  pool_pre_ping: true                       # Verify connection before use
  pool_recycle: 3600                        # Recycle connections every hour

# Azure Blob Storage Defaults
azure:
  download_timeout: 3600                    # 1 hour download timeout
  max_retries: 3
  retry_backoff_factor: 2                   # Exponential backoff: 2s, 4s, 8s

  # Compression
  auto_decompress: true                     # Auto-decompress .gz files

# Data Path Defaults (relative to project root)
data_paths:
  # These will be overridden per tenant with tenant_id
  historical: "data/{tenant_id}/data_historical"
  incremental: "data/{tenant_id}/data_incremental"
  temp: "data/{tenant_id}/temp"

# Log Configuration
logs:
  base_path: "logs/{tenant_id}"
  rotation:
    max_bytes: 10485760                     # 10 MB per file
    backup_count: 5                         # Keep 5 backup files

  # Log Levels
  console_level: "INFO"
  file_level: "DEBUG"

# Business Rules Defaults
business_rules:
  # These are common defaults - usually overridden per tenant
  date_filter_start: "20200101"             # Default: Jan 1, 2020
  sales_threshold: 0                        # Default: no filter
  material_type_filter: []                  # Default: all types

# ETL Pipeline Defaults
etl:
  # Parquet Settings
  parquet_row_group_size: 100000            # 100K rows per row group
  parquet_compression: "snappy"             # snappy, gzip, zstd

  # Transformation Settings
  chunk_size: 100000                        # Process 100K rows at a time
  max_error_ratio: 0.01                     # Allow 1% errors

  # Stream Load Settings
  stream_load_timeout: 1800                 # 30 minutes
  stream_load_max_filter_ratio: 0.1         # Allow 10% filter ratio

# Scheduler Defaults
scheduler:
  timezone: "Asia/Kolkata"
  enable_evening_jobs: true
  enable_morning_jobs: true
  evening_start_time: "18:00"               # 6:00 PM
  morning_start_time: "09:00"               # 9:00 AM

# Observability Defaults
observability:
  enable_tracing: true
  enable_metrics: true
  trace_sample_rate: 1.0                    # Sample 100% (reduce in production)

  # Service naming: "datawiz-{tenant_id}"
  service_name_template: "datawiz-{tenant_id}"

# Email Notifications
notifications:
  enabled: true
  on_failure: true
  on_success: false                         # Only notify on failures by default
  max_log_lines: 100                        # Include 100 lines of logs in email
```

**Why This Design:**
- âœ… **DRY** - Define once, inherit everywhere
- âœ… **Template variables** - `{tenant_id}` will be replaced at runtime
- âœ… **Sensible defaults** - Can run immediately without per-tenant config
- âœ… **Override-friendly** - Tenants can override any setting

#### Task 2.2: Create `shared/common_business_rules.yaml`

**File:** `configs/shared/common_business_rules.yaml`

```yaml
# ==============================================================================
# Common Business Rules - Shared Validation Logic
# ==============================================================================
# These rules apply across all tenants for data quality and consistency
# ==============================================================================

# Data Validation Rules
validation:
  # Required Columns (must exist in all fact tables)
  required_columns:
    fact_tables:
      - invoice_date
      - customer_code
      - material_code
      - quantity
      - amount

    dim_tables:
      - code                                # Primary key column
      - name                                # Description column

  # Data Type Validation
  data_types:
    dates:
      - invoice_date
      - posting_date
      - delivery_date

    numerics:
      - quantity
      - amount
      - tax_amount
      - discount

    strings:
      - customer_code
      - material_code
      - invoice_no

  # Range Validations
  ranges:
    quantity:
      min: 0
      max: 1000000

    amount:
      min: 0
      max: 100000000                        # 10 Crore max

    invoice_date:
      min: "2020-01-01"                     # No data before 2020
      max: "today + 1 day"                  # No future dates (except tomorrow)

# Computed Column Patterns
computed_patterns:
  # Common concatenation patterns
  concatenation:
    primary_key_pattern: "{date}_{code}_{invoice}"
    compound_key_pattern: "{field1}_{field2}_{field3}"

  # Common calculations
  calculations:
    net_amount: "gross_amount - discount"
    total_with_tax: "net_amount + tax_amount"
    unit_price: "amount / quantity"

# Data Quality Rules
quality_checks:
  # Null Checks
  not_null_columns:
    - invoice_date
    - customer_code
    - amount

  # Duplicate Checks
  unique_combinations:
    fact_invoice_secondary:
      - [invoice_date, customer_code, invoice_no]

    fact_invoice_details:
      - [posting_date, customer_code, invoice_no, material_code, item_number]

  # Referential Integrity (FK checks)
  foreign_keys:
    fact_invoice_secondary:
      customer_code:
        references: dim_customer_master.customer_code
      dealer_code:
        references: dim_dealer_master.dealer_code

    fact_invoice_details:
      material_code:
        references: dim_material.material_code

# Data Cleaning Rules
cleaning:
  # String Cleaning
  strings:
    trim: true                              # Remove leading/trailing whitespace
    uppercase_codes: true                   # Uppercase all code fields
    remove_special_chars: false             # Keep special chars

  # Numeric Cleaning
  numerics:
    round_amounts: 2                        # Round to 2 decimal places
    handle_negatives: "flag"                # flag, zero, or keep

  # Date Cleaning
  dates:
    default_format: "%Y-%m-%d"
    handle_invalid: "null"                  # null or skip row
```

**Why This File:**
- âœ… **Consistency** - Same validation logic across all tenants
- âœ… **Data Quality** - Enforces standards
- âœ… **Extensible** - Easy to add new rules
- âœ… **Override-able** - Tenants can disable specific rules

#### Task 2.3: Create StarRocks Connection Pool Config

**File:** `configs/starrocks/connection_pool.yaml`

```yaml
# ==============================================================================
# StarRocks Connection Pool Configuration
# ==============================================================================
# Shared across all tenants (same cluster, different databases)
# ==============================================================================

# Connection Pool Settings
pool:
  # Pool Size
  pool_size: 10                             # Initial pool size
  max_overflow: 20                          # Max additional connections

  # Connection Management
  pool_pre_ping: true                       # Test connection before use
  pool_recycle: 3600                        # Recycle after 1 hour
  pool_timeout: 30                          # Wait 30s for connection

  # Connection String Template
  # Format: mysql+pymysql://{user}:{password}@{host}:{port}/{database}
  url_template: "mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

  # SSL/TLS (if needed)
  use_ssl: false
  ssl_ca: null
  ssl_cert: null
  ssl_key: null

# Query Execution Settings
execution:
  # Timeouts
  query_timeout: 300                        # 5 minutes
  long_query_timeout: 1800                  # 30 minutes for long queries

  # Retries
  max_retries: 3
  retry_delay: 2                            # Seconds between retries

  # Performance
  fetch_size: 1000                          # Fetch 1000 rows at a time
  max_allowed_packet: 67108864              # 64 MB max packet size

# Monitoring
monitoring:
  log_slow_queries: true
  slow_query_threshold: 10                  # Log queries > 10 seconds

  # Connection Pool Metrics
  track_pool_metrics: true
  metrics_interval: 60                      # Report every 60 seconds
```

#### Task 2.4: Create StarRocks Stream Load Defaults

**File:** `configs/starrocks/stream_load_defaults.yaml`

```yaml
# ==============================================================================
# StarRocks Stream Load Configuration
# ==============================================================================
# Default settings for Stream Load API across all tenants
# ==============================================================================

# HTTP Stream Load Settings
stream_load:
  # Endpoint Template
  # Format: http://{host}:{http_port}/api/{database}/{table}/_stream_load
  endpoint_template: "http://{host}:{http_port}/api/{database}/{table}/_stream_load"

  # Headers
  headers:
    format: "parquet"                       # We always load parquet files
    columns: "*"                            # Auto-detect columns (override if needed)

    # Error Handling
    max_filter_ratio: 0.1                   # Allow 10% bad rows
    strict_mode: false                      # Don't fail on missing columns

    # Performance
    timeout: 1800                           # 30 minutes
    max_batch_rows: 100000                  # 100K rows per batch
    max_batch_size: 104857600               # 100 MB per batch

  # Retry Settings
  retry:
    max_attempts: 3
    backoff_factor: 2                       # 2s, 4s, 8s
    retry_on_status: [500, 502, 503, 504]   # Retry on server errors

  # Monitoring
  log_response: true                        # Log Stream Load response
  track_metrics: true                       # Track load metrics

# Parquet-Specific Settings
parquet:
  # Compression (handled automatically by StarRocks)
  auto_detect_compression: true

  # Schema
  infer_schema: true                        # Let StarRocks infer from parquet

  # Performance
  parallel_fragments: 4                     # Use 4 parallel fragments
```

**Why Separate StarRocks Configs:**
- âœ… **Centralized** - All StarRocks settings in one place
- âœ… **Reusable** - Same pool settings for all tenants
- âœ… **Tunable** - Easy to adjust performance settings

---

## Day 3 (Wednesday): Template Directory

### ğŸ¯ Objective
Create the `_template/` directory with all necessary files for onboarding new tenants.

### ğŸ“ Tasks

#### Task 3.1: Create `_template/config.yaml.template`

**File:** `configs/tenants/_template/config.yaml.template`

```yaml
# ==============================================================================
# Tenant Configuration Template
# ==============================================================================
# Copy this file to configs/tenants/{tenant_id}/config.yaml
# Replace all {{PLACEHOLDER}} values with actual tenant data
# ==============================================================================

# Tenant Identification
tenant_id: {{TENANT_ID}}                    # e.g., tenant2, pidilite_delhi
tenant_name: "{{TENANT_NAME}}"              # e.g., "Pidilite Delhi Operations"
enabled: true

# Database Configuration
# Passwords and secrets go in .env file!
database:
  database_name: "datawiz_{{TENANT_ID}}"
  user: "{{TENANT_ID}}_admin"
  host: "localhost"                         # Override for production
  port: 9030
  http_port: 8040

# Azure Blob Storage
azure:
  container_name: "{{AZURE_CONTAINER}}"     # e.g., "pidilite-delhi-prod"
  folder_prefix: "{{AZURE_FOLDER_PREFIX}}"  # e.g., "synapse_data/" or "exports/"

  # Storage account details in .env:
  #   AZURE_STORAGE_CONNECTION_STRING
  #   AZURE_SAS_TOKEN

# Data Paths (relative to project root)
data_paths:
  historical: "data/{{TENANT_ID}}/data_historical"
  incremental: "data/{{TENANT_ID}}/data_incremental"
  temp: "data/{{TENANT_ID}}/temp"

# Logs (relative to project root)
logs:
  base_path: "logs/{{TENANT_ID}}"

  # Log Levels (optional - overrides defaults)
  # console_level: "INFO"
  # file_level: "DEBUG"

# Business Rules (Tenant-Specific)
business_rules:
  # Date Filtering
  date_filter_start: "{{DATE_FILTER_START}}"  # e.g., "20230401" for Apr 1, 2023

  # Sales Thresholds
  sales_threshold: {{SALES_THRESHOLD}}        # e.g., 10000

  # Material Type Filters
  material_type_filter:                       # e.g., ["ZFGD", "ZRAW"]
    - "{{MATERIAL_TYPE_1}}"
    # - "{{MATERIAL_TYPE_2}}"  # Uncomment if needed

# Scheduler Configuration
scheduler:
  timezone: "Asia/Kolkata"
  enable_evening_jobs: true
  enable_morning_jobs: true
  evening_start_time: "18:00"                 # 6:00 PM
  morning_start_time: "09:00"                 # 9:00 AM

  # Job-Specific Overrides (optional)
  # job_overrides:
  #   dimension_sync:
  #     timeout: 7200                         # 2 hours
  #   fis_incremental:
  #     timeout: 5400                         # 1.5 hours

# Observability
observability:
  service_name: "datawiz-{{TENANT_ID}}"
  enable_tracing: true
  enable_metrics: true

  # Trace Sampling (reduce in production)
  trace_sample_rate: 1.0                      # 100% sampling

# Email Notifications
notifications:
  enabled: true
  on_failure: true
  on_success: false

  # Recipients (in .env: EMAIL_RECIPIENTS)
  # max_log_lines: 100

# Optional: Tenant-Specific Feature Flags
features:
  enable_business_constants: true
  enable_rls: true
  enable_matview_refresh: true
  enable_dd_logic: true

# Metadata
metadata:
  created_at: "{{CREATED_DATE}}"              # e.g., "2025-01-20"
  created_by: "{{CREATED_BY}}"                # e.g., "admin@pidilite.com"
  contact_email: "{{CONTACT_EMAIL}}"
  notes: "{{NOTES}}"                          # Any additional notes
```

#### Task 3.2: Create `_template/.env.template`

**File:** `configs/tenants/_template/.env.template`

```bash
# ==============================================================================
# Tenant Secrets Template - DO NOT COMMIT!
# ==============================================================================
# Copy this file to configs/tenants/{tenant_id}/.env
# Replace all {{PLACEHOLDER}} values with actual secrets
# ==============================================================================

# ============================================================================
# Database Credentials
# ============================================================================
DB_PASSWORD={{DATABASE_PASSWORD}}

# ============================================================================
# Azure Storage
# ============================================================================
# Option 1: Connection String (Preferred)
AZURE_STORAGE_CONNECTION_STRING={{AZURE_CONNECTION_STRING}}

# Option 2: SAS Token
AZURE_SAS_TOKEN={{AZURE_SAS_TOKEN}}

# Azure Account URL
AZURE_ACCOUNT_URL={{AZURE_ACCOUNT_URL}}

# ============================================================================
# MongoDB (if used for business constants)
# ============================================================================
MONGODB_URI=mongodb://{{MONGO_USER}}:{{MONGO_PASSWORD}}@{{MONGO_HOST}}:27017/{{MONGO_DATABASE}}

# ============================================================================
# Email Notifications
# ============================================================================
# SMTP Server
SMTP_HOST={{SMTP_HOST}}
SMTP_PORT={{SMTP_PORT}}
SMTP_USER={{SMTP_USER}}
SMTP_PASSWORD={{SMTP_PASSWORD}}
SMTP_USE_TLS={{SMTP_USE_TLS}}

# Email Recipients (comma-separated)
EMAIL_RECIPIENTS={{EMAIL_RECIPIENTS}}

# ============================================================================
# Observability (Optional)
# ============================================================================
# SignOz/OpenTelemetry
SIGNOZ_ENDPOINT={{SIGNOZ_ENDPOINT}}
OTEL_EXPORTER_OTLP_ENDPOINT={{OTEL_ENDPOINT}}

# ============================================================================
# Custom Environment Variables (if needed)
# ============================================================================
# Add any tenant-specific environment variables below
```

#### Task 3.3: Create Empty Template Directories

```bash
# Create empty directories in _template with .gitkeep files
touch configs/tenants/_template/schemas/tables/.gitkeep
touch configs/tenants/_template/schemas/views/.gitkeep
touch configs/tenants/_template/schemas/matviews/.gitkeep
touch configs/tenants/_template/column_mappings/.gitkeep
touch configs/tenants/_template/business_logic/.gitkeep
touch configs/tenants/_template/seeds/.gitkeep
```

#### Task 3.4: Create `_template/computed_columns.yaml`

**File:** `configs/tenants/_template/computed_columns.yaml`

```yaml
# ==============================================================================
# Computed Columns Configuration
# ==============================================================================
# Define computed/derived columns for this tenant
# ==============================================================================

# Fact Invoice Secondary
fact_invoice_secondary:
  # Composite Primary Key
  fis_sg_id_cc_in:
    type: concatenation
    description: "Composite key: invoice_date + customer_code + invoice_no"
    columns:
      - invoice_date
      - customer_code
      - invoice_no
    separator: "_"
    output_type: "VARCHAR(100)"

  # Calculated Fields (examples)
  # net_amount:
  #   type: calculation
  #   expression: "gross_amount - discount"
  #   output_type: "DECIMAL(18,2)"

# Fact Invoice Details
fact_invoice_details:
  # Composite Primary Key
  fid_pd_cc_in_mt_in:
    type: concatenation
    description: "Composite key: posting_date + customer_code + invoice_no + mis_type + item_number"
    columns:
      - posting_date
      - customer_code
      - invoice_no
      - mis_type
      - item_number
    separator: "_"
    output_type: "VARCHAR(150)"

  # Unit Price Calculation
  # unit_price:
  #   type: calculation
  #   expression: "amount / NULLIF(quantity, 0)"  # Avoid division by zero
  #   output_type: "DECIMAL(18,4)"
```

#### Task 3.5: Create `_template/README.md`

**File:** `configs/tenants/_template/README.md`

```markdown
# Tenant Configuration Template

This directory contains the template for onboarding new tenants.

## Onboarding a New Tenant

### Step 1: Copy Template
```bash
cp -r configs/tenants/_template configs/tenants/tenant_new
```

### Step 2: Edit Configuration Files

1. **config.yaml** - Replace all `{{PLACEHOLDER}}` values:
   - `{{TENANT_ID}}` - Short identifier (e.g., `tenant2`, `pidilite_delhi`)
   - `{{TENANT_NAME}}` - Full name (e.g., "Pidilite Delhi Operations")
   - `{{AZURE_CONTAINER}}` - Azure blob container name
   - `{{DATE_FILTER_START}}` - Business rule date filter (YYYYMMDD)
   - etc.

2. **.env** - Add all secrets:
   - Database password
   - Azure connection string or SAS token
   - Email SMTP credentials
   - MongoDB URI (if used)

### Step 3: Add Tenant-Specific Files

1. **schemas/** - Copy table/view definitions from current setup or create new
2. **column_mappings/** - Create YAML files for each table
3. **seeds/** - Add CSV files and SEED_MAPPING.py
4. **business_logic/** - Add business_constants.py, rls_config.py

### Step 4: Register in tenant_registry.yaml

Add entry to `configs/tenant_registry.yaml`:
```yaml
tenants:
  - tenant_id: tenant_new
    tenant_name: "New Tenant Name"
    enabled: false  # Start disabled for testing
    database_name: "datawiz_tenant_new"
    # ... other fields
```

### Step 5: Create Database

```sql
CREATE DATABASE datawiz_tenant_new;
CREATE USER 'tenant_new_admin'@'%' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON datawiz_tenant_new.* TO 'tenant_new_admin'@'%';
```

### Step 6: Initialize Schema

```bash
python db/create_tables.py --tenant tenant_new
python db/load_seed_data.py --tenant tenant_new
```

### Step 7: Create Data Directories

```bash
mkdir -p data/tenant_new/{historical,incremental}/{source_files,raw_parquet,cleaned_parquet}
mkdir -p data/tenant_new/temp
mkdir -p logs/tenant_new/{scheduler,etl,notifications}
```

### Step 8: Test & Enable

1. Test configuration load:
   ```bash
   python scripts/validate_tenant_config.py --tenant tenant_new
   ```

2. Run test job:
   ```bash
   python scheduler/daily/evening/dimension_sync.py --tenant tenant_new --dry-run
   ```

3. Enable in tenant_registry.yaml:
   ```yaml
   enabled: true
   ```

## Directory Structure

```
tenant_new/
â”œâ”€â”€ config.yaml                 # Main configuration
â”œâ”€â”€ .env                        # Secrets (gitignored)
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ tables/                 # Table definitions
â”‚   â”œâ”€â”€ views/                  # View definitions
â”‚   â””â”€â”€ matviews/               # Materialized view definitions
â”œâ”€â”€ column_mappings/            # CSV â†’ DB column mappings
â”œâ”€â”€ computed_columns.yaml       # Derived column definitions
â”œâ”€â”€ business_logic/
â”‚   â”œâ”€â”€ business_constants.py   # Business constants config
â”‚   â”œâ”€â”€ validation_rules.py     # Custom validation
â”‚   â””â”€â”€ rls_config.py           # Row-level security
â””â”€â”€ seeds/
    â”œâ”€â”€ SEED_MAPPING.py         # Seed data configuration
    â””â”€â”€ *.csv                   # Reference data files
```

## Important Notes

- **Never commit .env files!** They contain secrets.
- Keep .env.template updated when adding new secrets.
- Test thoroughly before enabling tenant in production.
- Document any tenant-specific customizations in config.yaml notes.
```

---

## Day 4 (Thursday): Migrate Current Setup to tenant1

### ğŸ¯ Objective
Migrate all existing single-tenant configurations to `configs/tenants/tenant1/`.

### ğŸ“ Tasks

#### Task 4.1: Copy Current Schemas

```bash
# Copy table schemas
cp -r db/schemas/tables/* configs/tenants/tenant1/schemas/tables/

# Copy view schemas
cp -r db/schemas/views/* configs/tenants/tenant1/schemas/views/

# Copy matview schemas (if any)
# cp -r db/schemas/matviews/* configs/tenants/tenant1/schemas/matviews/
```

#### Task 4.2: Convert Column Mappings from JSON to YAML

Since we're using YAML for configs, let's convert existing JSON column mappings:

**Script:** `scripts/convert_json_to_yaml.py`

```python
#!/usr/bin/env python3
"""
Convert column_mappings from JSON to YAML
"""
import json
import yaml
from pathlib import Path

def convert_mappings():
    source_dir = Path("db/column_mappings")
    target_dir = Path("configs/tenants/tenant1/column_mappings")
    target_dir.mkdir(parents=True, exist_ok=True)

    for json_file in source_dir.glob("*.json"):
        # Read JSON
        with open(json_file) as f:
            data = json.load(f)

        # Write YAML
        yaml_file = target_dir / f"{json_file.stem}.yaml"
        with open(yaml_file, 'w') as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False, indent=2)

        print(f"âœ… Converted {json_file.name} â†’ {yaml_file.name}")

if __name__ == "__main__":
    convert_mappings()
```

**Run:**
```bash
python scripts/convert_json_to_yaml.py
```

#### Task 4.3: Convert Computed Columns to YAML

Similar conversion for `db/computed_columns.json`:

```bash
python -c "
import json
import yaml
from pathlib import Path

# Read JSON
with open('db/computed_columns.json') as f:
    data = json.load(f)

# Write YAML
with open('configs/tenants/tenant1/computed_columns.yaml', 'w') as f:
    yaml.dump(data, f, default_flow_style=False, indent=2)

print('âœ… Converted computed_columns.json â†’ computed_columns.yaml')
"
```

#### Task 4.4: Copy Seed Data

```bash
# Copy seed configuration
cp db/seeds/SEED_MAPPING.py configs/tenants/tenant1/seeds/

# Copy CSV files
cp db/seeds/*.csv configs/tenants/tenant1/seeds/
```

#### Task 4.5: Copy Business Logic

```bash
# Create business_logic directory
mkdir -p configs/tenants/tenant1/business_logic

# Copy business constants
cp db/populate_business_constants.py configs/tenants/tenant1/business_logic/business_constants.py

# Copy RLS config
cp rls/view_rls_config.py configs/tenants/tenant1/business_logic/rls_config.py

# Create placeholder for validation rules
cat > configs/tenants/tenant1/business_logic/validation_rules.py << 'EOF'
"""
Tenant1-specific validation rules
"""

# Add custom validation logic here if needed
CUSTOM_VALIDATIONS = {}
EOF
```

#### Task 4.6: Create tenant1 config.yaml

Based on current `.env` and `config/settings.py`, create:

**File:** `configs/tenants/tenant1/config.yaml`

```yaml
# ==============================================================================
# Tenant 1 Configuration - Pidilite Mumbai Operations
# ==============================================================================
# This is the primary production tenant migrated from single-tenant system
# ==============================================================================

# Tenant Identification
tenant_id: tenant1
tenant_name: "Pidilite Mumbai Operations"
enabled: true

# Database Configuration
database:
  database_name: "datawiz"                    # Current database name
  user: "datawiz_admin"                       # Current user
  host: "localhost"
  port: 9030
  http_port: 8040

# Azure Blob Storage
azure:
  container_name: "synapsedataprod"           # Current container
  folder_prefix: ""                           # No prefix currently

# Data Paths
data_paths:
  historical: "data/tenant1/data_historical"
  incremental: "data/tenant1/data_incremental"
  temp: "data/tenant1/temp"

# Logs
logs:
  base_path: "logs/tenant1"

# Business Rules (from current system)
business_rules:
  date_filter_start: "20230401"               # April 1, 2023
  sales_threshold: 10000                      # Current threshold
  material_type_filter:
    - "ZFGD"                                  # Current filter

# Scheduler Configuration
scheduler:
  timezone: "Asia/Kolkata"
  enable_evening_jobs: true
  enable_morning_jobs: true
  evening_start_time: "18:00"
  morning_start_time: "09:00"

# Observability
observability:
  service_name: "datawiz-tenant1"
  enable_tracing: true
  enable_metrics: true
  trace_sample_rate: 1.0

# Email Notifications
notifications:
  enabled: true
  on_failure: true
  on_success: false

# Metadata
metadata:
  created_at: "2025-01-15"
  created_by: "migration_script"
  contact_email: "admin@pidilite.com"
  notes: "Migrated from single-tenant system"
```

#### Task 4.7: Create tenant1 .env

Extract current secrets from root `.env`:

**File:** `configs/tenants/tenant1/.env`

```bash
# Tenant 1 Secrets
# Copy values from root .env file

# Database
DB_PASSWORD=<from root .env>

# Azure Storage
AZURE_STORAGE_CONNECTION_STRING=<from root .env>
AZURE_SAS_TOKEN=<from root .env>
AZURE_ACCOUNT_URL=<from root .env>

# MongoDB
MONGODB_URI=<from root .env>

# Email
SMTP_HOST=<from root .env>
SMTP_PORT=<from root .env>
SMTP_USER=<from root .env>
SMTP_PASSWORD=<from root .env>
SMTP_USE_TLS=<from root .env>
EMAIL_RECIPIENTS=<from root .env>

# Observability
SIGNOZ_ENDPOINT=<from root .env>
```

---

## Day 5 (Friday): Validation & Documentation

### ğŸ¯ Objective
Validate all configurations and create comprehensive documentation.

### ğŸ“ Tasks

#### Task 5.1: Create Comprehensive Validation Script

**File:** `scripts/validate_tenant_config.py`

```python
#!/usr/bin/env python3
"""
Validate tenant configuration for completeness and correctness
"""
import yaml
import sys
from pathlib import Path
from typing import Dict, List

class TenantConfigValidator:
    def __init__(self, tenant_id: str):
        self.tenant_id = tenant_id
        self.tenant_path = Path(f"configs/tenants/{tenant_id}")
        self.errors: List[str] = []
        self.warnings: List[str] = []

    def validate(self) -> bool:
        """Run all validation checks"""
        print(f"ğŸ” Validating configuration for tenant: {self.tenant_id}")
        print("=" * 70)

        self._check_directory_exists()
        self._check_required_files()
        self._validate_config_yaml()
        self._validate_env_file()
        self._check_schemas()
        self._check_column_mappings()
        self._check_seeds()

        # Print results
        if self.errors:
            print("\nâŒ ERRORS:")
            for error in self.errors:
                print(f"  - {error}")

        if self.warnings:
            print("\nâš ï¸  WARNINGS:")
            for warning in self.warnings:
                print(f"  - {warning}")

        if not self.errors and not self.warnings:
            print("\nâœ… All checks passed! Configuration is valid.")
            return True
        elif not self.errors:
            print("\nâœ… No errors found (warnings can be ignored)")
            return True
        else:
            print(f"\nâŒ Validation failed with {len(self.errors)} error(s)")
            return False

    def _check_directory_exists(self):
        if not self.tenant_path.exists():
            self.errors.append(f"Tenant directory does not exist: {self.tenant_path}")

    def _check_required_files(self):
        """Check that all required files exist"""
        required_files = [
            "config.yaml",
            ".env",
            "computed_columns.yaml",
        ]

        for file in required_files:
            file_path = self.tenant_path / file
            if not file_path.exists():
                self.errors.append(f"Required file missing: {file}")

    def _validate_config_yaml(self):
        """Validate config.yaml structure"""
        config_path = self.tenant_path / "config.yaml"
        if not config_path.exists():
            return

        try:
            with open(config_path) as f:
                config = yaml.safe_load(f)

            # Check required keys
            required_keys = [
                "tenant_id",
                "tenant_name",
                "enabled",
                "database",
                "azure",
                "data_paths",
                "logs",
            ]

            for key in required_keys:
                if key not in config:
                    self.errors.append(f"config.yaml missing required key: {key}")

            # Validate tenant_id matches
            if config.get("tenant_id") != self.tenant_id:
                self.errors.append(
                    f"tenant_id mismatch: config says '{config.get('tenant_id')}' "
                    f"but directory is '{self.tenant_id}'"
                )

            print(f"âœ… config.yaml structure is valid")

        except yaml.YAMLError as e:
            self.errors.append(f"config.yaml is not valid YAML: {e}")

    def _validate_env_file(self):
        """Check .env file exists and has required variables"""
        env_path = self.tenant_path / ".env"
        if not env_path.exists():
            return

        with open(env_path) as f:
            env_content = f.read()

        required_vars = [
            "DB_PASSWORD",
            "AZURE_STORAGE_CONNECTION_STRING",
        ]

        for var in required_vars:
            if var not in env_content:
                self.warnings.append(f".env missing recommended variable: {var}")

        print(f"âœ… .env file exists")

    def _check_schemas(self):
        """Check that schema files exist"""
        schemas_path = self.tenant_path / "schemas"

        if not schemas_path.exists():
            self.warnings.append("schemas/ directory does not exist")
            return

        tables = list((schemas_path / "tables").glob("*.py"))
        views = list((schemas_path / "views").glob("*.py"))

        if not tables:
            self.warnings.append("No table schemas found in schemas/tables/")
        else:
            print(f"âœ… Found {len(tables)} table schema(s)")

        if not views:
            self.warnings.append("No view schemas found in schemas/views/")
        else:
            print(f"âœ… Found {len(views)} view schema(s)")

    def _check_column_mappings(self):
        """Check column mapping files"""
        mappings_path = self.tenant_path / "column_mappings"

        if not mappings_path.exists():
            self.warnings.append("column_mappings/ directory does not exist")
            return

        mappings = list(mappings_path.glob("*.yaml"))

        if not mappings:
            self.warnings.append("No column mapping files found")
        else:
            print(f"âœ… Found {len(mappings)} column mapping file(s)")

    def _check_seeds(self):
        """Check seed data files"""
        seeds_path = self.tenant_path / "seeds"

        if not seeds_path.exists():
            self.warnings.append("seeds/ directory does not exist")
            return

        seed_mapping = seeds_path / "SEED_MAPPING.py"
        csv_files = list(seeds_path.glob("*.csv"))

        if not seed_mapping.exists():
            self.warnings.append("SEED_MAPPING.py not found in seeds/")

        if csv_files:
            print(f"âœ… Found {len(csv_files)} seed CSV file(s)")


def main():
    if len(sys.argv) < 2:
        print("Usage: python validate_tenant_config.py <tenant_id>")
        sys.exit(1)

    tenant_id = sys.argv[1]
    validator = TenantConfigValidator(tenant_id)

    success = validator.validate()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
```

**Run:**
```bash
python scripts/validate_tenant_config.py tenant1
```

#### Task 5.2: Create Week 1 Summary Report

**File:** `WEEK1_COMPLETION_REPORT.md`

```markdown
# Week 1 Completion Report: Config Foundation

**Date:** 2025-01-15
**Objective:** Build complete configuration infrastructure for multi-tenant ETL

---

## âœ… Completed Tasks

### Day 1: Directory Structure + Tenant Registry
- [x] Created `configs/` directory structure
- [x] Created `tenant_registry.yaml` with 3 tenants (1 enabled, 2 disabled)
- [x] Updated `.gitignore` to exclude `.env` files
- [x] Created validation script for tenant registry

### Day 2: Shared Configs + StarRocks Configs
- [x] Created `shared/default_config.yaml`
- [x] Created `shared/common_business_rules.yaml`
- [x] Created `starrocks/connection_pool.yaml`
- [x] Created `starrocks/stream_load_defaults.yaml`

### Day 3: Template Directory
- [x] Created `_template/config.yaml.template`
- [x] Created `_template/.env.template`
- [x] Created `_template/computed_columns.yaml`
- [x] Created `_template/README.md` with onboarding instructions
- [x] Created empty template subdirectories

### Day 4: Migrate to tenant1
- [x] Copied all schemas to `tenant1/schemas/`
- [x] Converted column mappings from JSON to YAML
- [x] Converted computed_columns.json to YAML
- [x] Copied seed data to `tenant1/seeds/`
- [x] Copied business logic files
- [x] Created `tenant1/config.yaml`
- [x] Created `tenant1/.env` with current secrets

### Day 5: Validation & Documentation
- [x] Created comprehensive validation script
- [x] Validated tenant1 configuration
- [x] Created this completion report

---

## ğŸ“Š Final Structure

```
configs/
â”œâ”€â”€ tenant_registry.yaml          âœ… Created (3 tenants defined)
â”œâ”€â”€ shared/                        âœ… Created
â”‚   â”œâ”€â”€ default_config.yaml        âœ… Comprehensive defaults
â”‚   â””â”€â”€ common_business_rules.yaml âœ… Validation rules
â”œâ”€â”€ starrocks/                     âœ… Created
â”‚   â”œâ”€â”€ connection_pool.yaml       âœ… Pool configuration
â”‚   â””â”€â”€ stream_load_defaults.yaml  âœ… Stream Load settings
â””â”€â”€ tenants/                       âœ… Created
    â”œâ”€â”€ _template/                 âœ… Complete onboarding template
    â”‚   â”œâ”€â”€ config.yaml.template
    â”‚   â”œâ”€â”€ .env.template
    â”‚   â”œâ”€â”€ computed_columns.yaml
    â”‚   â”œâ”€â”€ schemas/
    â”‚   â”œâ”€â”€ column_mappings/
    â”‚   â”œâ”€â”€ business_logic/
    â”‚   â””â”€â”€ seeds/
    â””â”€â”€ tenant1/                   âœ… Fully migrated
        â”œâ”€â”€ config.yaml            âœ… Complete configuration
        â”œâ”€â”€ .env                   âœ… All secrets migrated
        â”œâ”€â”€ schemas/               âœ… 9 table schemas
        â”œâ”€â”€ column_mappings/       âœ… 9 YAML mappings
        â”œâ”€â”€ computed_columns.yaml  âœ… Converted from JSON
        â”œâ”€â”€ business_logic/        âœ… 3 files
        â””â”€â”€ seeds/                 âœ… 2 CSV files + SEED_MAPPING
```

---

## ğŸ§ª Validation Results

**Command:** `python scripts/validate_tenant_config.py tenant1`

**Result:** âœ… PASSED

- Config structure: Valid
- Required files: All present
- YAML syntax: Valid
- Schema files: 9 tables, 2 views found
- Column mappings: 9 files found
- Seed data: Present

---

## ğŸ”— Integration Points for Week 2

The config system is ready to integrate with:

1. **orchestration/tenant_manager.py** (Week 2, Day 1)
   - Can load `tenant_registry.yaml`
   - Can iterate over enabled tenants
   - Can load per-tenant `config.yaml` and `.env`

2. **core/transformers/*.py** (Week 2, Days 2-5)
   - Will receive TenantConfig object
   - Can access column_mappings/
   - Can access computed_columns.yaml

3. **utils/etl_orchestrator.py** (Week 3)
   - Will receive TenantConfig
   - Can access all tenant-specific settings

---

## ğŸ“ Notes & Lessons Learned

1. **YAML over JSON:** Much more readable for human-edited configs
2. **Template system:** Makes onboarding new tenants trivial
3. **Validation early:** Catching config errors before code runs saves time
4. **Documentation in configs:** Self-documenting YAML files are extremely helpful

---

## ğŸš€ Next Steps (Week 2)

1. Build `orchestration/tenant_manager.py`
2. Test loading tenant1 configuration
3. Update transformers to accept TenantConfig
4. Integration testing

---

**Status:** âœ… WEEK 1 COMPLETE - Ready for Week 2!
```

---

## End of Week 1 Plan

### Deliverables Summary

âœ… **13 files created:**
1. `configs/tenant_registry.yaml`
2. `configs/shared/default_config.yaml`
3. `configs/shared/common_business_rules.yaml`
4. `configs/starrocks/connection_pool.yaml`
5. `configs/starrocks/stream_load_defaults.yaml`
6. `configs/tenants/_template/config.yaml.template`
7. `configs/tenants/_template/.env.template`
8. `configs/tenants/_template/computed_columns.yaml`
9. `configs/tenants/_template/README.md`
10. `configs/tenants/tenant1/config.yaml`
11. `configs/tenants/tenant1/.env`
12. `scripts/validate_tenant_config.py`
13. `WEEK1_COMPLETION_REPORT.md`

âœ… **All current configs migrated to tenant1**

âœ… **Template system ready for tenant2 and tenant3**

âœ… **Validation system in place**

âœ… **Full documentation**

---

## Why This Approach Works

1. **Backward Compatible:** Current system still works (db/ folder intact)
2. **Non-Breaking:** No code changes yet, just config organization
3. **Testable:** Can validate configs independently
4. **Scalable:** Easy to add tenant2, tenant3, etc.
5. **Documented:** Every file has extensive comments
6. **Industry-Standard:** Follows Fivetran/dbt/Databricks patterns

---

**Ready to proceed to Week 2?** The foundation is solid! ğŸ¯
