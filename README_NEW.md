# ğŸ­ Pidilite DataWiz - ETL Pipeline

Modern, modular ETL pipeline for loading data from Azure Blob Storage into StarRocks database with comprehensive monitoring and notifications.

## âœ¨ Features

- **Modular Architecture**: Clean separation of concerns (Extract, Transform, Load)
- **StarRocks Database**: High-performance OLAP database for analytics
- **Azure Integration**: Direct blob storage integration
- **Automated Scheduling**: Evening (dimensions) and morning (facts) jobs
- **Email Notifications**: Alerts for job success/failure
- **Observability**: OpenTelemetry tracing, SignOz metrics, comprehensive logging
- **Incremental Loading**: Checkpoint-based processing for efficiency
- **Data Quality**: Validation and cleaning at every stage

## ğŸ“ Project Structure

```
pidilite_datawiz/
â”‚
â”œâ”€â”€ config/                   # ğŸ”§ Configuration Management
â”‚   â”œâ”€â”€ settings.py           # Centralized settings (DB, Azure, Email)
â”‚   â”œâ”€â”€ database.py           # StarRocks connection pooling
â”‚   â”œâ”€â”€ storage.py            # Azure Blob Storage client
â”‚   â”œâ”€â”€ logging_config.py     # Logging configuration
â”‚   â””â”€â”€ observability.py      # OpenTelemetry tracing & metrics
â”‚
â”œâ”€â”€ core/                     # ğŸš€ ETL Pipeline
â”‚   â”œâ”€â”€ extractors/           # Extract from Azure Blob Storage
â”‚   â”œâ”€â”€ transformers/         # Clean, validate, transform data
â”‚   â”œâ”€â”€ loaders/              # Load to StarRocks (batch/incremental)
â”‚   â””â”€â”€ jobs/                 # ETL job definitions
â”‚
â”œâ”€â”€ scheduler/                # â° Job Scheduling
â”‚   â”œâ”€â”€ daily/evening/        # 6 PM - Load dimension tables
â”‚   â”œâ”€â”€ daily/morning/        # 8 AM - Load fact tables
â”‚   â”œâ”€â”€ monthly/              # Monthly maintenance
â”‚   â””â”€â”€ crontab.yaml          # Job schedule configuration
â”‚
â”œâ”€â”€ notifications/            # ğŸ“§ Email & Alerts
â”‚   â”œâ”€â”€ email_service.py      # SMTP email service
â”‚   â”œâ”€â”€ templates/            # HTML email templates
â”‚   â””â”€â”€ utils/                # Email generators (MJML)
â”‚
â”œâ”€â”€ db/                       # ğŸ—„ï¸ Database Management
â”‚   â”œâ”€â”€ tables.py             # Table definitions
â”‚   â”œâ”€â”€ migrations/           # Schema migrations
â”‚   â”œâ”€â”€ indexes/              # Index optimization
â”‚   â””â”€â”€ rls/                  # Row-level security
â”‚
â”œâ”€â”€ observability/            # ğŸ“Š Monitoring
â”‚   â”œâ”€â”€ tracer.py             # OpenTelemetry tracing
â”‚   â”œâ”€â”€ metrics.py            # Custom metrics
â”‚   â””â”€â”€ dashboards/           # SignOz dashboards
â”‚
â”œâ”€â”€ data/                     # ğŸ’¾ Data Storage
â”‚   â”œâ”€â”€ data_historical/      # Historical data (one-time load)
â”‚   â””â”€â”€ data_incremental/     # Incremental daily data
â”‚
â”œâ”€â”€ logs/                     # ğŸ“ Application Logs
â”‚   â”œâ”€â”€ scheduler/            # Scheduler logs
â”‚   â”œâ”€â”€ etl/                  # ETL job logs
â”‚   â””â”€â”€ notifications/        # Email notification logs
â”‚
â””â”€â”€ docs/                     # ğŸ“š Documentation
    â”œâ”€â”€ QUICK_START.md        # Getting started guide
    â”œâ”€â”€ migration_guide.md    # Migration instructions
    â””â”€â”€ ARCHITECTURE.md       # System architecture
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Copy environment template
cp .env.example .env

# Edit with your credentials
nano .env
```

### 2. Install Dependencies

```bash
pip install -e .
```

### 3. Configure Database

Update `.env` with your StarRocks credentials:

```env
DB_HOST=your_starrocks_host
DB_PORT=9030
DB_NAME=datawiz
DB_USER=root
DB_PASSWORD=your_password
```

### 4. Configure Azure Storage

```env
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;...
AZURE_CONTAINER_NAME=your-container
```

### 5. Initialize Database

```bash
python db/create_tables.py
python db/indexes/clean_indexes.py
```

### 6. Start Pipeline

```bash
# Manual test run
python scheduler/daily/evening/main.py   # Load dimensions
python scheduler/daily/morning/main.py   # Load facts

# Start automated scheduler
python scheduler/orchestrator.py
```

## ğŸ“Š ETL Flow

### Evening Pipeline (6:00 PM - 8:00 PM)
**Load Dimension Tables First**

```
18:00  Dimension Sync        â†’ Load reference/master data
18:30  TSR Hierarchy Update  â†’ Update sales hierarchies
19:00  Refresh Mat Views     â†’ Update reporting views
19:30  Business Constants    â†’ Update MongoDB configs
```

### Morning Pipeline (8:00 AM - 12:00 PM)
**Load Fact Tables (depends on dimensions)**

```
08:00  Blob Backup           â†’ Download latest Azure files
09:00  FIS Incremental Load  â†’ Load sales fact table
10:30  FID Incremental Load  â†’ Load distribution fact
12:00  DD Logic              â†’ Business calculations
```

## ğŸ¯ Key Components

### Configuration ([config/](config/))
- **Centralized settings**: All config in one place
- **Connection pooling**: Reusable database connections
- **Environment-based**: Separate dev/prod configs

### ETL Pipeline ([core/](core/))
- **Extractors**: Download from Azure Blob Storage
- **Transformers**: Clean, validate, map data
- **Loaders**: Batch/incremental inserts to StarRocks
- **Jobs**: Orchestrated ETL workflows

### Scheduler ([scheduler/](scheduler/))
- **YAML-based**: Easy configuration (crontab.yaml)
- **Automatic retry**: Failed jobs retry automatically
- **Dependencies**: Morning jobs wait for evening completion

### Notifications ([notifications/](notifications/))
- **Email alerts**: Job success/failure notifications
- **Daily reports**: Summary of all jobs
- **MJML templates**: Beautiful HTML emails

### Observability ([observability/](observability/))
- **OpenTelemetry**: Distributed tracing
- **SignOz**: Metrics and dashboards
- **Structured logging**: Searchable logs

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|------------|
| Database | StarRocks (OLAP) |
| Storage | Azure Blob Storage |
| Language | Python 3.9+ |
| Scheduler | APScheduler |
| Monitoring | OpenTelemetry + SignOz |
| Data Format | Parquet (columnar) |
| Testing | pytest |

## ğŸ“– Documentation

- **[Quick Start Guide](docs/QUICK_START.md)** - Get up and running
- **[Migration Guide](docs/migration_guide.md)** - Migrate existing pipelines
- **[Architecture](docs/ARCHITECTURE.md)** - System design details
- **[Project Summary](PROJECT_SUMMARY.md)** - Overview and next steps

## ğŸ” Common Tasks

### Manually Trigger a Job

```bash
# Test dimension sync
python -m scheduler.daily.evening.dimension_sync

# Test fact load
python -m scheduler.daily.morning.fis_incremental
```

### Check Logs

```bash
# Scheduler logs
tail -f logs/scheduler/scheduler.log

# ETL job logs
tail -f logs/etl/etl.log

# All logs
tail -f logs/**/*.log
```

### Validate Configuration

```bash
# Test database connection
python config/database.py

# Test Azure connection
python config/storage.py

# Validate all config
python config/settings.py
```

## ğŸ“§ Email Notifications

Configure in `.env`:

```env
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@example.com
SMTP_PASSWORD=your_app_password
EMAIL_RECIPIENTS=admin@pidilite.com,team@pidilite.com
```

**Notifications sent for:**
- Job failures (immediate)
- Daily summary (8 PM)
- Data validation errors
- System health alerts

## ğŸ“ˆ Monitoring & Observability

### Logs
- Location: `logs/` directory
- Rotation: Daily, 30-day retention
- Format: Structured JSON logs

### Tracing (OpenTelemetry)
- Trace every job execution
- Measure execution time
- Identify bottlenecks

### Metrics (SignOz)
- Job success/failure rates
- Data volume processed
- Database performance
- System resources

## ğŸ”’ Security

- âœ… Environment-based credentials (.env)
- âœ… No hardcoded passwords
- âœ… Connection pooling (secure connections)
- âœ… Row-level security (RLS) support
- âœ… TLS encryption for emails

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Unit tests only
pytest tests/unit/

# Integration tests
pytest tests/integration/

# With coverage
pytest --cov
```

## ğŸ“¦ Data Flow

```
Azure Blob Storage
    â†“ [Extract]
data/*/raw/
    â†“ [Transform]
data/*/cleaned_parquets/
    â†“ [Validate]
data/*/incremental/
    â†“ [Load]
StarRocks Database
    â†“ [Verify]
Email Notification
```

## ğŸ“ Best Practices

1. **Always load dimensions before facts**
   - Evening jobs (dimensions) run first
   - Morning jobs (facts) depend on dimension data

2. **Monitor disk space**
   - Archive old files regularly
   - Clean temp directories

3. **Test before production**
   - Use `.env.dev` for testing
   - Validate data quality

4. **Review logs regularly**
   - Check for warnings
   - Monitor job execution times

## ğŸ› Troubleshooting

### Database Connection Failed
```bash
# Check StarRocks is running
telnet your_starrocks_host 9030

# Verify config
python config/database.py
```

### Azure Access Error
```bash
# Test connection
python config/storage.py

# List containers
az storage container list --connection-string "..."
```

### Jobs Not Running
```bash
# Check scheduler enabled
grep ENABLE_SCHEDULER .env

# View logs
tail -f logs/scheduler/scheduler.log
```

## ğŸ¤ Support

- **Documentation**: See `docs/` directory
- **Logs**: Check `logs/` directory
- **Issues**: Contact team

## ğŸ“ License

MIT License - See LICENSE file

## ğŸ¯ Next Steps

1. âœ… Setup complete - Review [QUICK_START.md](docs/QUICK_START.md)
2. âš™ï¸ Configure environment - Edit `.env` file
3. ğŸ—„ï¸ Initialize database - Run `python db/create_tables.py`
4. ğŸš€ Start pipeline - Run `python scheduler/orchestrator.py`
5. ğŸ“Š Monitor - Check logs and metrics

---

**Built with â¤ï¸ for Pidilite DataWiz**
